# ðŸš¢ MLOps Project 3: Titanic Survival Prediction

This project is part of the MLOps Project Series and focuses on implementing **data engineering**, **feature stores**, and **ML monitoring** concepts using a well-known dataset: the Titanic Survival dataset.

While the machine learning task is familiar, the emphasis of this project is on integrating key MLOps components in a realistic workflow.

---

![MLOps Project 3 Workflow](mlops-project3_revised.png)

---

## ðŸŽ¯ Objective

To predict passenger survival using the Titanic dataset and to integrate advanced MLOps practices such as:

- ETL pipelines using **Airflow (Astronomer)**
- Feature Store using **Redis**
- ML monitoring with **Prometheus** and **Grafana**
- Data Drift Detection using **EvidentlyAI** or **LeviDetect**

---

## ðŸ§± Project Highlights

### âœ… ETL Pipeline with Airflow
- Build Extract-Transform-Load pipelines using **Astronomer Airflow**
- Extract Titanic CSV data from GCP buckets (simulated)
- Transform it into table-compatible formats
- Load into **PostgreSQL**

### âœ… Feature Store with Redis
- Store processed features (X_train, Y_train, etc.)
- Avoid repeated transformations and improve scalability
- Use Redis CLI/Docker for feature store setup

### âœ… Data Drift Detection
- Compare incoming data with training reference data
- Use **LeviDetect** or **EvidentlyAI** to track and alert on drift

### âœ… ML Monitoring with Prometheus & Grafana
- Define custom metrics: prediction count, data drift count, mean of age, etc.
- Visualize them in real-time dashboards using Grafana
- Automate alerts (e.g., trigger retraining if drift > threshold)

---

## ðŸ”„ Workflow Breakdown

1. **Database Setup**: Simulated cloud storage using GCP buckets and PostgreSQL
2. **Project Setup**: Virtual env, folder structure, `setup.py`, `requirements.txt`
3. **ETL Pipeline**: Astronomer Airflow to extract-transform-load data into PostgreSQL
4. **Data Ingestion**: Read from PostgreSQL, save to CSV, split into train/test
5. **Jupyter Notebook Testing**: EDA, feature engineering, baseline model
6. **Feature Store**: Push clean features to Redis; later pull for model training
7. **Data Processing & Feature Extraction**: Modular pipeline code using `src/`
8. **Model Training**: Train classifier using features pulled from Redis
9. **Training Pipeline**: Combine data ingestion, processing, and training into one script
10. **Versioning**: Use GitHub for both code and small dataset versioning
11. **User App (Flask)**: Build web app for input/output; stylize with HTML + ChatGPT-generated CSS
12. **Data Drift Detection**: Compare live inputs with training data
13. **ML Monitoring**: Use Prometheus to log metrics; Grafana for dashboards

---

## ðŸ“ Dataset

- **Source**: [Kaggle - Titanic Survival Dataset](https://www.kaggle.com/competitions/titanic/data)
- **Fields**: Name, Age, Pclass, SibSp, Cabin, Embarked, etc.
- **Target**: `Survived` (0 = No, 1 = Yes)

---

## ðŸ› ï¸ Tools & Libraries

| Category          | Tools / Libraries              |
|-------------------|--------------------------------|
| ML & EDA          | Pandas, Scikit-learn, Matplotlib |
| ETL               | Apache Airflow (Astronomer)    |
| Feature Store     | Redis (via Docker)             |
| Monitoring        | Prometheus, Grafana            |
| Drift Detection   | LeviDetect / EvidentlyAI       |
| Web Framework     | Flask + HTML/CSS               |
| Versioning        | GitHub                         |
| Database          | PostgreSQL                     |

---

## ðŸ’¡ Why Titanic Dataset?

This dataset is widely understood and avoids the need to spend time on:
- Understanding new feature spaces
- Complex model architecture
- Unknown preprocessing techniques

This allows full focus on mastering **ETL pipelines**, **Feature Store**, and **Monitoring**, which are **crucial in real-world production systems**.

---

## ðŸ“Œ Key MVPs (Milestones)

1. **ETL Pipeline with Airflow**
2. **Redis-based Feature Store**
3. **Data Drift Detection**
4. **ML Monitoring with Grafana + Prometheus**

---

## ðŸ“ Next Steps

Start with the `database_setup/` and `etl_pipeline/` folders, and follow the numbered modules.

> âœ… Tip: Revisit MLOps Project 1 & 2 for a refresher on data ingestion and training pipeline basics.

---

## ðŸ§  Bonus Ideas (Advanced Users)

- Integrate CI/CD using Jenkins or GitHub Actions
- Deploy trained Docker image to GCP Cloud Run or Kubernetes
- Extend monitoring with model performance (precision, recall, etc.)

---

## ðŸ™Œ Acknowledgements

Special thanks to the open-source tools and contributors of:
- [Astronomer.io](https://www.astronomer.io/)
- [Redis](https://redis.io/)
- [Grafana Labs](https://grafana.com/)
- [Prometheus](https://prometheus.io/)

---

> ðŸš€ Letâ€™s get started and build production-grade ML workflows with confidence!
